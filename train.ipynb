{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"train.ipynb","provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyOz5J9LozStoAriuZtPGfPz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jUnOjmVupwFm","executionInfo":{"status":"ok","timestamp":1622454554150,"user_tz":-540,"elapsed":23129,"user":{"displayName":"Akira Kawabata","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgELYUA4Tg07Wq1UMrlNJM2gc0gtwW-jqSkcIYT=s64","userId":"00796758475895835901"}},"outputId":"dc9315e3-e79b-4cbb-ec5b-a0106c8ec620"},"source":["\n","import numpy as np\n","import pandas as pd\n","import pickle\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torchtext\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MNPj2j8lsvKX","executionInfo":{"status":"ok","timestamp":1622454584707,"user_tz":-540,"elapsed":30562,"user":{"displayName":"Akira Kawabata","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgELYUA4Tg07Wq1UMrlNJM2gc0gtwW-jqSkcIYT=s64","userId":"00796758475895835901"}},"outputId":"b37954e6-665a-4e5a-8594-56adcdc6678a"},"source":["!pip install transformers[ja]"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Collecting transformers[ja]\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d5/43/cfe4ee779bbd6a678ac6a97c5a5cdeb03c35f9eaebbb9720b036680f9a2d/transformers-4.6.1-py3-none-any.whl (2.2MB)\n","\u001b[K     |████████████████████████████████| 2.3MB 6.3MB/s \n","\u001b[?25hCollecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n","\u001b[K     |████████████████████████████████| 901kB 26.1MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers[ja]) (2.23.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers[ja]) (3.0.12)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers[ja]) (4.0.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers[ja]) (20.9)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers[ja]) (4.41.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers[ja]) (2019.12.20)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers[ja]) (1.19.5)\n","Collecting huggingface-hub==0.0.8\n","  Downloading https://files.pythonhosted.org/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl\n","Collecting tokenizers<0.11,>=0.10.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n","\u001b[K     |████████████████████████████████| 3.3MB 34.1MB/s \n","\u001b[?25hCollecting fugashi>=1.0; extra == \"ja\"\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/55/9c/009da34dd111e84f54eef833c84afb5c744a0306af8546014a958e1967a0/fugashi-1.1.0-cp37-cp37m-manylinux1_x86_64.whl (486kB)\n","\u001b[K     |████████████████████████████████| 491kB 61.7MB/s \n","\u001b[?25hCollecting ipadic<2.0,>=1.0.0; extra == \"ja\"\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e7/4e/c459f94d62a0bef89f866857bc51b9105aff236b83928618315b41a26b7b/ipadic-1.0.0.tar.gz (13.4MB)\n","\u001b[K     |████████████████████████████████| 13.4MB 27.8MB/s \n","\u001b[?25hCollecting unidic-lite>=1.0.7; extra == \"ja\"\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/55/2b/8cf7514cb57d028abcef625afa847d60ff1ffbf0049c36b78faa7c35046f/unidic-lite-1.0.8.tar.gz (47.4MB)\n","\u001b[K     |████████████████████████████████| 47.4MB 111kB/s \n","\u001b[?25hCollecting unidic>=1.0.2; extra == \"ja\"\n","  Downloading https://files.pythonhosted.org/packages/86/04/c18832fd9959a78fc60eeaa9e7fb37ef31a250e8645cc2897eb1f07939ee/unidic-1.0.3.tar.gz\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers[ja]) (1.0.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers[ja]) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers[ja]) (7.1.2)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[ja]) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[ja]) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[ja]) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[ja]) (2020.12.5)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers[ja]) (3.7.4.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers[ja]) (3.4.1)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers[ja]) (2.4.7)\n","Requirement already satisfied: wasabi<1.0.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from unidic>=1.0.2; extra == \"ja\"->transformers[ja]) (0.8.2)\n","Requirement already satisfied: plac<2.0.0,>=1.1.3 in /usr/local/lib/python3.7/dist-packages (from unidic>=1.0.2; extra == \"ja\"->transformers[ja]) (1.1.3)\n","Building wheels for collected packages: ipadic, unidic-lite, unidic\n","  Building wheel for ipadic (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ipadic: filename=ipadic-1.0.0-cp37-none-any.whl size=13556725 sha256=acaff8ea781d6a50b891e227756a8e783baf8b71aec049da48a1c3bb6e920f81\n","  Stored in directory: /root/.cache/pip/wheels/ff/00/d1/0c094a0ce58a77199a0c5801f0ecf510c80f0ecbec27f07d2c\n","  Building wheel for unidic-lite (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for unidic-lite: filename=unidic_lite-1.0.8-cp37-none-any.whl size=47658825 sha256=b46cd6fef20b1195ec182b24566638b000153989a2e721e76d971f181f4dba9d\n","  Stored in directory: /root/.cache/pip/wheels/20/48/8d/b66d8361a27f58f41ec86640e4fd2640de0403a6367511eab7\n","  Building wheel for unidic (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for unidic: filename=unidic-1.0.3-cp37-none-any.whl size=5497 sha256=f694e3f896f3d30ac19c95c30356a8a8ef833d06383f7a90f003d73f01875c1b\n","  Stored in directory: /root/.cache/pip/wheels/d3/26/e2/fb76c79fd14391eb994eab021c9129c24814125298e1e5b96a\n","Successfully built ipadic unidic-lite unidic\n","Installing collected packages: sacremoses, huggingface-hub, tokenizers, fugashi, ipadic, unidic-lite, unidic, transformers\n","Successfully installed fugashi-1.1.0 huggingface-hub-0.0.8 ipadic-1.0.0 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.6.1 unidic-1.0.3 unidic-lite-1.0.8\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0cj4Sc4kp2cv","executionInfo":{"status":"ok","timestamp":1622454586286,"user_tz":-540,"elapsed":1583,"user":{"displayName":"Akira Kawabata","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgELYUA4Tg07Wq1UMrlNJM2gc0gtwW-jqSkcIYT=s64","userId":"00796758475895835901"}}},"source":["df = pd.read_csv('drive/MyDrive/tweet_data_cleaned.csv')\n","df = df[['text', 'emo']]\n","train_df, test_df = train_test_split(df, test_size = 0.2)\n","train_df.to_csv('drive/MyDrive/train.tsv', sep='\\t', index=False, header=None)\n","test_df.to_csv('drive/MyDrive/test.tsv', sep='\\t', index=False, header=None)"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"ghc6nuMKqFYp","executionInfo":{"status":"ok","timestamp":1622455720495,"user_tz":-540,"elapsed":10746,"user":{"displayName":"Akira Kawabata","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgELYUA4Tg07Wq1UMrlNJM2gc0gtwW-jqSkcIYT=s64","userId":"00796758475895835901"}}},"source":["\n","from transformers import BertModel\n","from transformers import AutoTokenizer\n","\n","tokenizer = AutoTokenizer.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')\n","\n","\n","MAX_LENGTH = 512\n","def bert_tokenizer(text):\n","    return tokenizer.encode(text, max_length=MAX_LENGTH, truncation=True, return_tensors='pt')[0]\n","\n","TEXT = torchtext.legacy.data.Field(sequential=True, tokenize=bert_tokenizer, use_vocab=False, lower=False,\n","                            include_lengths=True, batch_first=True, fix_length=MAX_LENGTH, pad_token=0)\n","LABEL = torchtext.legacy.data.Field(sequential=False, use_vocab=False)\n","\n","train_data, test_data = torchtext.legacy.data.TabularDataset.splits(\n","    path='drive/MyDrive', train='train.tsv', test='test.tsv', format='tsv', fields=[('Text', TEXT), ('Label', LABEL)])\n","\n","\n","BATCH_SIZE = 32\n","train_iter, test_iter = torchtext.legacy.data.Iterator.splits((train_data, test_data), batch_sizes=(BATCH_SIZE, BATCH_SIZE), repeat=False, sort=False)"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4s1IgDMgruia","executionInfo":{"status":"ok","timestamp":1622479106197,"user_tz":-540,"elapsed":2066,"user":{"displayName":"Akira Kawabata","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgELYUA4Tg07Wq1UMrlNJM2gc0gtwW-jqSkcIYT=s64","userId":"00796758475895835901"}},"outputId":"9b1341de-c89a-4f0f-f7ac-c8fa21ee7675"},"source":["\n","class BertClassifier(nn.Module):\n","    def __init__(self):\n","        super(BertClassifier, self).__init__()\n","\n","\n","        self.bert = BertModel.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking',\n","                                              output_attentions=True,\n","                                              output_hidden_states=True)\n","\n","\n","        self.linear = nn.Linear(768*4, 3)\n","\n","\n","        nn.init.normal_(self.linear.weight, std=0.02)\n","        nn.init.normal_(self.linear.bias, 0)\n","\n","    def _get_cls_vec(self, vec):\n","        return vec[:,0,:].view(-1, 768)\n","\n","    def forward(self, input_ids):\n","\n","        output = self.bert(input_ids)\n","        attentions = output['attentions']\n","        hidden_states = output['hidden_states']\n","\n","\n","        vec1 = self._get_cls_vec(hidden_states[-1])\n","        vec2 = self._get_cls_vec(hidden_states[-2])\n","        vec3 = self._get_cls_vec(hidden_states[-3])\n","        vec4 = self._get_cls_vec(hidden_states[-4])\n","\n","\n","        vec = torch.cat([vec1, vec2, vec3, vec4], dim=1)\n","\n","\n","        out = self.linear(vec)\n","\n","        return F.log_softmax(out, dim=1), attentions\n","\n","classifier = BertClassifier()"],"execution_count":29,"outputs":[{"output_type":"stream","text":["Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"x3VBt6aOr0Y8","executionInfo":{"status":"ok","timestamp":1622479106198,"user_tz":-540,"elapsed":5,"user":{"displayName":"Akira Kawabata","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgELYUA4Tg07Wq1UMrlNJM2gc0gtwW-jqSkcIYT=s64","userId":"00796758475895835901"}}},"source":["\n","\n","for param in classifier.parameters():\n","    param.requires_grad = False\n","\n","\n","for param in classifier.bert.encoder.layer[-1].parameters():\n","    param.requires_grad = True\n","\n","for param in classifier.bert.encoder.layer[-2].parameters():\n","    param.requires_grad = True\n","\n","for param in classifier.bert.encoder.layer[-3].parameters():\n","    param.requires_grad = True\n","\n","for param in classifier.bert.encoder.layer[-4].parameters():\n","    param.requires_grad = True\n","\n","\n","for param in classifier.linear.parameters():\n","    param.requires_grad = True\n","\n","\n","optimizer = optim.Adam([\n","    {'params': classifier.bert.encoder.layer[-1].parameters(), 'lr': 5e-5},\n","    {'params': classifier.bert.encoder.layer[-2].parameters(), 'lr': 5e-5},\n","    {'params': classifier.bert.encoder.layer[-3].parameters(), 'lr': 5e-5},\n","    {'params': classifier.bert.encoder.layer[-4].parameters(), 'lr': 5e-5},\n","    {'params': classifier.linear.parameters(), 'lr': 1e-4}\n","])"],"execution_count":30,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zpp3GVokr46v","executionInfo":{"status":"ok","timestamp":1622494948702,"user_tz":-540,"elapsed":15840917,"user":{"displayName":"Akira Kawabata","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgELYUA4Tg07Wq1UMrlNJM2gc0gtwW-jqSkcIYT=s64","userId":"00796758475895835901"}},"outputId":"f5876152-5548-4836-df05-5011792c3aa1"},"source":["\n","\n","loss_function = nn.NLLLoss()\n","\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","classifier.to(device)\n","losses = []\n","epoch = 20\n","\n","for epoch in range(epoch):\n","\n","\n","    all_loss = 0\n","\n","    for idx, batch in enumerate(train_iter):\n","\n","      classifier.zero_grad()\n","\n","      input_ids = batch.Text[0].to(device)\n","      label_ids = batch.Label.to(device)\n","\n","      out, _ = classifier(input_ids)\n","\n","      batch_loss = loss_function(out, label_ids)\n","      batch_loss.backward()\n","\n","      optimizer.step()\n","\n","      all_loss += batch_loss.item()\n","\n","    print(\"epoch\", epoch, \"\\t\" , \"loss\", all_loss)\n","\n"],"execution_count":31,"outputs":[{"output_type":"stream","text":["epoch 0 \t loss 609.8599146604538\n","epoch 1 \t loss 417.34804433584213\n","epoch 2 \t loss 344.3662644326687\n","epoch 3 \t loss 260.9938557544956\n","epoch 4 \t loss 154.42577229253948\n","epoch 5 \t loss 78.00698536528216\n","epoch 6 \t loss 72.48712145430545\n","epoch 7 \t loss 45.1747447671944\n","epoch 8 \t loss 33.422685147073935\n","epoch 9 \t loss 35.1499112306758\n","epoch 10 \t loss 29.4158189808295\n","epoch 11 \t loss 30.049728129495634\n","epoch 12 \t loss 26.719187616454292\n","epoch 13 \t loss 26.19840474901457\n","epoch 14 \t loss 27.402458135446068\n","epoch 15 \t loss 21.397829050030055\n","epoch 16 \t loss 21.015190969090327\n","epoch 17 \t loss 22.124865675563342\n","epoch 18 \t loss 21.15882081655309\n","epoch 19 \t loss 20.39051566930901\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uHP1Ekiy0o7Z","executionInfo":{"status":"ok","timestamp":1622495071580,"user_tz":-540,"elapsed":122884,"user":{"displayName":"Akira Kawabata","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgELYUA4Tg07Wq1UMrlNJM2gc0gtwW-jqSkcIYT=s64","userId":"00796758475895835901"}},"outputId":"e828a788-7480-41d6-e1bb-72bbc42a2550"},"source":["\n","answer = []\n","prediction = []\n","\n","with torch.no_grad():\n","    for batch in test_iter:\n","\n","        text_tensor = batch.Text[0].to(device)\n","        label_tensor = batch.Label.to(device)\n","\n","        score, _ = classifier(text_tensor)\n","        _, pred = torch.max(score, 1)\n","\n","        prediction += list(pred.cpu().numpy())\n","        answer += list(label_tensor.cpu().numpy())\n","\n","print(classification_report(prediction, answer))\n"],"execution_count":32,"outputs":[{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.72      0.84      0.77      2045\n","           1       0.87      0.74      0.80      2751\n","           2       0.82      0.84      0.83      2389\n","\n","    accuracy                           0.80      7185\n","   macro avg       0.80      0.81      0.80      7185\n","weighted avg       0.81      0.80      0.80      7185\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0m6DkPLwN8kZ","executionInfo":{"status":"ok","timestamp":1622495074143,"user_tz":-540,"elapsed":2569,"user":{"displayName":"Akira Kawabata","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgELYUA4Tg07Wq1UMrlNJM2gc0gtwW-jqSkcIYT=s64","userId":"00796758475895835901"}}},"source":["torch.save(classifier.state_dict(), 'drive/MyDrive/bert_tweet_gpu_20_2.pth')\n","torch.save(classifier.to('cpu').state_dict(), 'drive/MyDrive/bert_tweet_cpu_20_2.pth')\n"],"execution_count":33,"outputs":[]},{"cell_type":"code","metadata":{"id":"T-5mvl8sNoYZ"},"source":[""],"execution_count":null,"outputs":[]}]}